---
title: "542 Deliverable 2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#row.names(fromPy) <-fromPy$US.States
#fromPy$US.States=NULL
#head(fromPy)
#scale(fromPy)

## R Markdown

link = 'https://github.com/Ditha123/governance_analytics/raw/main/Deliverable%201/alldatafinal.csv'
myFile=url(link)
fromPy=read.csv(link)
str(fromPy)
head(fromPy)
##setting row names



#Custering 
selection= c("US.States","HS.Degree..","Bachelors.Degree..","Advanced.Degree..","Median.Household.Income")
dataToCluster=fromPy[,selection]
##Setting Row Names
row.names(dataToCluster)=dataToCluster$US.States
dataToCluster$US.States=NULL
head(dataToCluster)



#scaling data
dataToCluster=scale(dataToCluster)
colnames(dataToCluster)



##Data has been transformmed:scalling
boxplot(dataToCluster,horizontal = T, las=2,cex.axis=0.4)

#Compute the DISTANCE MATRIX:
set.seed(999) # this is for replicability of results

library(cluster)
dataToCluster_DM=daisy(x=dataToCluster, metric = "gower")

#Computer CLUSTERS
library(factoextra)


###CLustering
#for partitioning
fviz_nbclust(dataToCluster, 
             pam,
             diss=dataToCluster_DM,
             method = "gap_stat",
              k.max = 10,verbose = F)
##for hierarchial
fviz_nbclust(dataToCluster, 
             hcut,
             diss=dataToCluster_DM,
             method = "gap_stat",
             k.max = 10,
             verbose = F,
             hc_func = "agnes")

##for divisive
fviz_nbclust(dataToCluster, 
             hcut,
             diss=dataToCluster_DM,
             method = "gap_stat",
             k.max = 10,
             verbose = F,
             hc_func = "diana")

NumberOfClusterDesired=2

set.seed(31)
# function to compute total within-cluster sum of squares
#Elbow Method k=2 as half the variance is explained by 2 variables 
fviz_nbclust(dataToCluster, kmeans, method = "wss", k.max = 10) + theme_minimal() + ggtitle("the Elbow Method")

##Gap Statistic k=1
gap_stat <- clusGap(dataToCluster, FUN = kmeans, nstart = 30, K.max = 10, B = 50)
fviz_gap_stat(gap_stat) + theme_minimal() + ggtitle("fviz_gap_stat: Gap Statistic")

#Silloette Method the desired K is 2
fviz_nbclust(dataToCluster, kmeans, method = "silhouette", k.max = 10) + theme_minimal() + ggtitle("The Silhouette Plot")

#selected k=2 as by 2 methods, half the variance is explained by only 2 components
NumberOfClusterDesired=2

# Partitioning technique
res.pam = pam(x=dataToCluster_DM,
              k = NumberOfClusterDesired,
              cluster.only = F)

# Hierarchical technique- agglomerative approach

library(factoextra)
res.agnes= hcut(dataToCluster_DM, 
                k = NumberOfClusterDesired,
                isdiss=TRUE,
                hc_func='agnes',
                hc_method = "ward.D2")

# Hierarchical technique- divisive approach
res.diana= hcut(dataToCluster_DM, 
                k = NumberOfClusterDesired,
                isdiss=TRUE,
                hc_func='diana',
                hc_method = "ward.D2")
                


fromPy
#2. Clustering results

#ADDING TO ORGINAL DF
fromPy
fromPy$pam=as.factor(res.pam$clustering)
fromPy$agn=as.factor(res.agnes$cluster)
fromPy$dia=as.factor(res.diana$cluster)


#row.names(fromPy)=fromPy$US.States
#fromPy$US.States=NULL
#fromPy

colnames(fromPy)
#VERIFY ORDINARY CLUSTERS

##Verify

aggregate(data=fromPy,
          Average.Life.Expectancy~pam,
          FUN=mean)
          
aggregate(data=fromPy,
        Average.Life.Expectancy~agn,
         FUN=mean)
aggregate(data=fromPy,
        Average.Life.Expectancy~dia,
          FUN=mean)

##recoding variables 
fromPy$pam=dplyr::recode_factor(fromPy$pam, 
                  `1` = '4',`2`='3',`3`='2',`4`='1')
fromPy$agn=dplyr::recode_factor(fromPy$agn, 
                  `1` = '4',`2`='3',`3`='2',`4`='1')
fromPy$dia=dplyr::recode_factor(fromPy$dia, 
                  `1` = '4',`2`='3',`3`='2',`4`='1')
                  
                  fromPy
##Recoding and attaching data to original datac
fromPy$pam=dplyr::recode_factor(fromPy$pam, 
                  `1` = '4',`2`='3',`3`='2',`4`='1')
fromPy$agn=dplyr::recode_factor(fromPy$agn, 
                  `1` = '4',`2`='3',`3`='2',`4`='1')
fromPy$dia=dplyr::recode_factor(fromPy$dia, 
                  `1` = '4',`2`='3',`3`='2',`4`='1')
                  
                  fromPy


#from factoextra
library(factoextra)

install.packages("MASS")
install.packages("dplyr")
install.packages("minpack.lm")
install.packages ("rgl")
install.packages ("robustbase")
install.packages ("Matrix")
install.packages("qpcR")
install.packages ("fviz")

#Plot silhouettes
library (factoextra)

fviz_silhouette(res.pam)
fviz_silhouette(res.agnes)
fviz_silhouette(res.diana)



##What is not factored properly 
head(data.frame(res.pam$silinfo$widths),10)

pamEval=data.frame(res.pam$silinfo$widths)
agnEval=data.frame(res.agnes$silinfo$widths)
diaEval=data.frame(res.diana$silinfo$widths)

pamPoor=rownames(pamEval[pamEval$sil_width<0,])
agnPoor=rownames(agnEval[agnEval$sil_width<0,])
diaPoor=rownames(diaEval[diaEval$sil_width<0,])

library("qpcR") 
options(rgl.useNULL = TRUE)
library(rgl)

##Identify States that are not clustered
bap_Clus=as.data.frame(qpcR:::cbind.na(sort(pamPoor), sort(agnPoor),sort(diaPoor)))
names(bap_Clus)=c("pam","agn","dia")
bap_Clus



###The recommened clusters for this data set is only 1 by the gap statistic ,which means that every element in my dataset belongs to one cluster. Based on the silloette and elbox methods, the recommended cluser size is 2. 

projectedData = cmdscale(dataToCluster_DM, k=2)
# save coordinates to original data frame:

fromPy$dim1 = projectedData[,1]
fromPy$dim2 = projectedData[,2]

# save coordinates to original data frame:
fromPy[,c('dim1','dim2')][1:10,]

####map 
base= ggplot(data=fromPy,
             aes(x=dim1, y=dim2,
                 label=US.States)) 
base + geom_text(size=2)

# see some:
fromPy[,c('dim1')][1:10,]

base= ggplot(data=fromPy,
             aes(x=dim1, y=dim2,
                 label=US.States)) 
base + geom_text(size=2)

#Color the map using the labels from PAM:
pamPlot=base + labs(title = "PAM") + geom_point(size=2,
                                              aes(color=pam),
                                              show.legend = T) 
                                            
#Color the map using the labels from Hierarchical AGNES:
agnPlot=base + labs(title = "AGNES") + geom_point(size=2,
                                              aes(color=agn),
                                              show.legend = T) 
#Color the map using the labels from Hierarchical DIANA:
  diaPlot=base + labs(title = "DIANA") + geom_point(size=2,
                                              aes(color=dia),
                                              show.legend = T)                                            
##Comparing Clustering visually
library(ggpubr)
ggarrange(pamPlot, agnPlot, diaPlot,ncol = 3,common.legend = T)

##Preparing Labels
# If name of country in black list, use it, else get rid of it
LABELpam=ifelse(fromPy$US.States%in%pamPoor,fromPy$US.States,"")
LABELdia=ifelse(fromPy$US.States%in%diaPoor,fromPy$US.States,"")
LABELagn=ifelse(fromPy$US.States%in%agnPoor,fromPy$US.States,"")

library(ggrepel)
##Pam Plat
pamPlot + geom_text_repel(aes(label=LABELpam))
#Diana Plot
diaPlot + geom_text_repel(aes(label=LABELdia))
#Agnes PLot
agnPlot + geom_text_repel(aes(label=LABELagn))

#AGNES Approach
fviz_dend(res.agnes,k=NumberOfClusterDesired, cex = 0.45, horiz = T,main = "AGNES approach")
#Diana Approach
fviz_dend(res.diana,k=NumberOfClusterDesired, cex = 0.45, horiz = T,main = "DIANA approach")


##PCA
selection= c("US.States","HS.Degree..","Bachelors.Degree..","Advanced.Degree..","Median.Household.Income")
dataForFA=fromPy[,selection]

##Setting Row Names
row.names(dataForFA)=dataForFA$US.States
dataForFA$US.States=NULL

##Scaling Data
head(dataForFA)
dataForFA=scale(dataForFA)

#install.packages ("lavaan")
#library(lavaan)

##Model using Lavaan
model='
educ_LE=~HS.Degree..+Bachelors.Degree..+Advanced.Degree..+Median.Household.Income
'
##identifying Fit
fit<-cfa(model, data = dataForFA,std.lv=TRUE)
indexCFA=lavPredict(fit)

##resetting Index
indexCFA[1:10]

##removing negatives and forcing index between 0-10
library(scales)
indexCFANorm=rescale(as.vector(indexCFA), 
                     to = c(0, 10))
indexCFANorm[1:10]

fromPy$demo_FA=indexCFANorm


#Comparing Index with Original Score 
base=ggplot(data=fromPy,
            aes(x=demo_FA,y=c(scale(Average.Life.Expectancy))))
base+geom_point()

##Evaluation MEasures for Life Expectancy
evalCFA1=parameterEstimates(fit, standardized =TRUE)
evalCFA1[evalCFA1$op=="=~",c('rhs','std.all','pvalue')]

##Coefficiencts 
evalCFA2=as.list(fitMeasures(fit))
evalCFA2[c("chisq", "df", "pvalue")] ##Chi2 value is high, 6.074

evalCFA2$tli # > 0.90 ##Tucker Lewis Value > 0.90

evalCFA2[c( 'rmsea.ci.lower','rmsea','rmsea.ci.upper')] ##rmse between 0.01-0.04, average at 0.2 means the model can relatively predict well 

##Plot of the residuals 
library(semPlot)
semPaths(fit, what='std', nCharNodes=0, sizeMan=12,
         edge.label.cex=1.5, fade=T,residuals = F)
         
##Regrssion
### Can we Predict Life Expectancy based on the catagoricals 
install.packages ("lattice")
install.packages ("ggplot2")
install.packages ("caret")
library (caret)

set.seed (123)
##Hypo 1:Life Expectancy Increases as Income Increases
hypo1=formula(scale(Average.Life.Expectancy)~ scale(Median.Household.Income))
##Hypo 2: Life Expectancy Increases as Educational Attainment Increases 
hypo2=formula(scale(Average.Life.Expectancy)~ scale(Advanced.Degree..) + scale(Bachelors.Degree..))


selection=c("US.States","Average.Life.Expectancy","HS.Degree..","Bachelors.Degree..","Advanced.Degree.." , "Median.Household.Income" )    
#fromPy_Scaled=fromPy[,selection]

##Scaled Data Set 
#row.names(fromPy_Scaled)=fromPy_Scaled$US.States
#fromPy_Scaled
#fromPy_Scaled['US.States']=NULL
#fromPy_Scaled=scale(fromPy_Scaled)
#head(fromPy_Scaled

##Predictive Approach
##Predictive Approach




selection = createDataPartition(scale(fromPy$Average.Life.Expectancy),
                                p = 0.75,
                                list = FALSE)
                                
trainGauss = fromPy[ selection, ]
testGauss  = fromPy[-selection, ]

###both Models have High r2, regression test of 5 random samples 
ctrl = trainControl(method = 'cv',number = 5)

##Hypo 1
gauss1CV = train(hypo1,
                 data = trainGauss, 
                 method = 'glm',
                 trControl = ctrl)

gauss1CV ###High R2 Score almost 0.61

gauss2CV = train(hypo2,
                 data = trainGauss, 
                 method = 'glm',
                 trControl = ctrl)

gauss2CV ##High R2Score, almost 0.72 
##High R2 shows it is good for predicting 

predictedVal<-predict(gauss2CV,testGauss)

postResample(obs = scale(testGauss$Average.Life.Expectancy),
             pred=predictedVal)

###R2 post sample is still high--0.72 almost )



         



When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
